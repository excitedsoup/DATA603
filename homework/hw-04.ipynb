{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "306edecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import FloatType, StructType, StringType, TimestampType, StructField, IntegerType\n",
    "from pyspark.sql.functions import UserDefinedFunction, col, when, to_timestamp, to_utc_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58457818",
   "metadata": {},
   "source": [
    "1. Specify the schema for the crime data set. (https://sparkbyexamples.com/pyspark/pyspark-structtype-and-structfield/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "15f3c65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crime_schema = StructType([\n",
    "    StructField('X', FloatType()),\n",
    "    StructField('Y', FloatType()),\n",
    "    StructField('RowID', IntegerType()),\n",
    "    StructField('CrimeDateTime', StringType()),\n",
    "    StructField('CrimeCode', StringType()),\n",
    "    StructField('Location', StringType()),\n",
    "    StructField('Description', StringType()),\n",
    "    StructField('Inside_Outside', StringType()),\n",
    "    StructField('Weapon', StringType()),\n",
    "    StructField('Post', StringType()),\n",
    "    StructField('District', StringType()),\n",
    "    StructField('Neighborhood', StringType()),\n",
    "    StructField('Latitude', FloatType()),\n",
    "    StructField('Longitude', FloatType()),\n",
    "    StructField('Geolocation', FloatType()),\n",
    "    StructField('Premise', StringType()),\n",
    "    StructField('VRIName', StringType()),\n",
    "    StructField('Total_Incidents', StringType())])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b590652d",
   "metadata": {},
   "source": [
    "2. Read the file using the schema definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "acacc394",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read.options(header = 'True').schema(crime_schema).csv('Part1_Crime_data.csv')\n",
    "name = 'CrimeDateTime'\n",
    "#udf = UserDefinedFunction(lambda x: x[:-3]+'.000'.replace('/', '-'), StringType())\n",
    "#df1 = df.select(*[udf(column).alias(name) if column == name else column for column in df.columns])\n",
    "#df2 = df1.withColumn('CrimeDateTime',col('CrimeDateTime').cast(TimestampType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0846102",
   "metadata": {},
   "source": [
    "3. Cache the DataFrame (https://sparkbyexamples.com/spark/spark-dataframe-cache-and-persist-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d206f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec822db3",
   "metadata": {},
   "source": [
    "4. Show the count of the rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2c0961ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22992/1706065354.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\dataframe.py\u001b[0m in \u001b[0;36mcount\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    662\u001b[0m         \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    663\u001b[0m         \"\"\"\n\u001b[1;32m--> 664\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    666\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    115\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 117\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    118\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"C:\\Users\\Peter\\anaconda3\\Lib\\site-packages\\pyspark\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 473, in main\nException: Python in worker has different version 3.9 than that in driver 3.8, PySpark cannot run with different minor versions. Please check environment variables PYSPARK_PYTHON and PYSPARK_DRIVER_PYTHON are correctly set.\n"
     ]
    }
   ],
   "source": [
    "df1.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5756b5",
   "metadata": {},
   "source": [
    "5. Print the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd22dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6172af55",
   "metadata": {},
   "source": [
    "6. Display first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c799b077",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6f09e",
   "metadata": {},
   "source": [
    "1. What are distinct crime codes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e178d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('crime_codes')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT DISTINCT CrimeCode\n",
    "FROM crime_codes\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac0694",
   "metadata": {},
   "source": [
    "2. Count the number of crimes by the crime codes and order by the resulting counts in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd8df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('crime_code_count')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT CrimeCode, COUNT(*) as count\n",
    "FROM crime_code_count\n",
    "GROUP BY CrimeCode\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e47d65",
   "metadata": {},
   "source": [
    "3. Which neighborhood had most crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2fee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('nb_most_crimes')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT Neighborhood, COUNT(*) as count\n",
    "FROM nb_most_crimes\n",
    "GROUP BY Neighborhood\n",
    "ORDER BY count\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c4910",
   "metadata": {},
   "source": [
    "4. Which month of the year had most crimes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09664e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('most_crimes')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT SUBSTRING(CrimeDateTime, 6, 2) as m, COUNT(*) as count\n",
    "FROM most_crimes\n",
    "GROUP BY m\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(12, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d64b1e2",
   "metadata": {},
   "source": [
    "5. What weapons were used? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fed499",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('weapons_used')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT Weapon\n",
    "FROM weapons_used\n",
    "WHERE Weapon is not null and Weapon != 'NA'\n",
    "GROUP BY Weapon\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6be303",
   "metadata": {},
   "source": [
    "6. Which weapon was used the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db09d645",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView('most_used_weapon')\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT Weapon, COUNT(*) as count\n",
    "FROM most_used_weapon\n",
    "WHERE Weapon is not null and Weapon != 'NA'\n",
    "GROUP BY Weapon\n",
    "ORDER BY count DESC\n",
    "\"\"\"\n",
    "output = spark.sql(query)\n",
    "output.show(1, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f6af91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
